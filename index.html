<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description"
          content="Bringing Depth-supervision to Dynamic Neural Radiance Fields with Scene Flows">
    <meta name="author" content="Claire Chen,
				 Mani Deep Cherukuri">

    <title>Bringing Depth-supervision to Dynamic Neural Radiance Fields with Scene Flows</title>
    <!-- Bootstrap core CSS -->
    <link href="css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom styles for this template -->
    <link href="css/offcanvas.css" rel="stylesheet">

<!--    <link rel="icon" href="img/favicon.gif" type="image/gif">-->
	<style>
	* {
	  box-sizing: border-box;
	}

	.column {
	  float: left;
	  width: 33.33%;
	  padding: 5px;
	}
	.column1 {
	  float: left;
	  width: 50.00%;
	  padding: 5px;
	}
	/* Clearfix (clear floats) */
	.row::after {
	  content: "";
	  clear: both;
	  display: table;
	}
	.responsive {
	  width: 100%;
	  height: auto;
	}
	</style>
</head>

<body>
<div class="container">

    <div class="jumbotron">
	    <h2>Bringing Depth-supervision to Dynamic Neural Radiance Fields with Scene Flows</h2>
<!-- 	    <h2>University of Minnesota</h2> -->
        <p class="abstract">Minnesota Robotics Institute, University of Minnesota</p>
        <p iclass="authors">
            <a>Claire Chen</a>,
            <a>Mani Deep Cherukuri</a>
        </p>

        <p>
            <a class="btn btn-primary" href="https://arxiv.org/abs/2107.02791">DS Paper</a>
            <a class="btn btn-primary" href="https://arxiv.org/abs/2011.13084">NSF Paper</a>
	    <a class="btn btn-primary" href="https://google.com">Code</a>
            <a class="btn btn-primary" href="https://drive.google.com/drive/folders/1OkYgeRcIcLOFu1ft5mRODWNQaPJ0ps90?usp=sharing">Datasets</a>
            <a class="btn btn-primary" href="docs/poster.pdf">Poster</a>
            <a class="btn btn-primary" href="https://drive.google.com/file/d/1KerkXYDJSgl0DOb3IRL3UqCeq3Z3vkqG/view?usp=sharing">Talk</a>
            <a class="btn btn-primary" href="https://rpm-lab.github.io/CSCI5980-Spr23-DeepRob/assets/slides/minn_deeprob_18_NeRF_variants.pdf">Slides</a>
<!--             <a class="btn btn-primary" href="https://drive.google.com/open?id=1IdOywOSLuK6WlkO5_h-ykr3ubeY9eDig">Trained Models</a> -->
    </div>

    <h2>Architecture</h2>
    <hr>
<!-- 	    <div class="col-sm-3"> -->
		<img src='docs/Framework.png' width="900" height="400" class="responsive">
<!-- 	    </div> -->
<!--     <iframe width="884" height="497" class='center' src="https://drive.google.com/file/d/1PQl-FZiWqzs7C13x_Apo7r-yXyHO-GvJ/view?usp=sharing" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe> -->

    <hr>
    <p>
	The objective of our work is to investigate the impact of depth-supervised loss on both training time and 
	performance in generation of 3D realistic models using NeRF. While the Depth-supervised NeRF paper demonstrated
	the effectiveness of depth-supervised loss in reducing training time for static scenes, our study aims to 
	evaluate whether this approach can be applied to dynamic scenes. To achieve this goal, we incorporated the 
	DS-loss into the Neural Scene Flow Fields model	and conducted experiments with different loss combinations.
    </p>

    <div class="section">
	<h2>Static View Synthesis</h2>
	<hr>
	<h4> <center> 2 view input  </center> </h4>
	    <div class='row vspace-top'>
		<div class="col-sm-6">
                <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                     <source src="docs/DSresults/fern_rgb.mp4" type="video/mp4">
                </video>
		</div>
		<div class="col-sm-6">
		<video width="100%" playsinline="" autoplay="" loop="" preload="" muted="" class="gif">
		    <source src="docs/DSresults/fern_disp.mp4" type="video/mp4">
		</video>
		</div>
		<div class="col-sm-6">
                <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                     <source src="docs/DSresults/fortress_rgb.mp4" type="video/mp4">
                </video>
		</div>
		<div class="col-sm-6">
		<video width="100%" playsinline="" autoplay="" loop="" preload="" muted="" class="gif">
		    <source src="docs/DSresults/fortress_disp.mp4" type="video/mp4">
		</video>
		</div>
	    </div>
        <p>
            Using just 2 view inputs and the sparse 3D points generated by the COLMAP, the Depth Supervised model is able to 
	    achieve training faster by a magnitude of 1 without compromising the quality of the render. 
        </p>
	
	<h4> <center> 5 view input on Gopher Data </center> </h4>
	    <div class='row'>
	    <div class="column">
		<img src='docs/DSresults/Goldy_img.jpg' style="width:100%" >
	    </div>
	    <div class="column">
		<img src='docs/DSresults/Goldy_sparse_pts.png' style="width:100%" >
	    </div>
	    <div class="column">
                <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                     <source src="docs/DSresults/gopher_5v_rgb_1.mp4" type="video/mp4" style="width:100%">
                </video>
	    </div>
	    </div>

        <p>
            The above is the results on the Gopher data collected by team 1. Unlike basic NeRF which just uses the input views, we also make use 
	    of the sparse 3d points generated during the COLMAP run for calculating the depth supervision loss. 
        </p>
	    
	    
        <h2>Dynamic View Synthesis </h2>
        <hr>
	<h4> <center> No depth loss </center> </h4>
	<div class='row'>
	<div class="column1">
	<img src='docs/NSFresults/None_rgb.gif' style="width:100%" >
	</div>
	<div class="column1">
	<img src='docs/NSFresults/None_dep.png' style="width:100%" >
	</div>
	</div>

	<h4> <center> Depth loss only  </center> </h4>
	<div class='row'>
	<div class="column1">
	<img src='docs/NSFresults/ds_rgb.gif' style="width:100%" >
	</div>
	<div class="column1">
	<img src='docs/NSFresults/ds_dep.png' style="width:100%" >
	</div>
   	</div> 
	    
	<h4> <center> Single View Depth loss  </center> </h4>
	<div class='row'>
	<div class="column1">
	<img src='docs/NSFresults/Mono_rgb.gif' style="width:100%" >
	</div>
	<div class="column1">
	<img src='docs/NSFresults/Mono_dep.png' style="width:100%" >
	</div>
	</div>    
	    
	<h4> <center> Mix (depth loss + single view depth)  </center> </h4>
	<div class='row'>
	<div class="column1">
	<img src='docs/NSFresults/mix_rgb.gif' style="width:100%" >
	</div>
	<div class="column1">
	<img src='docs/NSFresults/mix_dep.png' style="width:100%" >
	</div>
	</div>
        <p>
	    As the depth supervised loss is incorporated in the total loss, we observe a significant improvement in the depth maps on the right. 
<!--             SRNs explain all 2D observations in 3D, leading to unsupervised, yet explicit, reconstruction of geometry jointly with
            appearance. Normal maps may visualize the reconstructed geometry and make SRNs fully interpretable.
            On the left, you can see the normal maps of the reconstructed
            geometry - note that these are learned fully unsupervised! In the center, you can see novel views generated
            by SRNs, and to the right, the ground-truth views. This model was trained on 50 2D observations each of ~2.5k cars in the Shapenet v2 dataset. -->
        </p>
        <hr>

        <h2>Performance Evaluation</h2>
	<div class='row'>
	<div class="column1">
	<img src='docs/NSFresults/4cases_Plot1.png' style="width:100%" >
	</div>
	<div class="column1">
	<img src='docs/NSFresults/4cases_Plot2.png' style="width:100%" >
	</div>
	</div>
        <p>
		Here, we compare the performance v/s time of all the models used for rendering a dynamic scene. We can observe that the loss for
		the mix model which incorporates the Single view depth + depth supervision converges faster when compared to other loss functions.
<!--             Here, we show first results for novel view synthesis. We rendered 500 images of a minecraft room,
            and trained a single SRN with 500k parameters on this dataset. -->
        </p>
	    
        <hr>
	<img src='docs/NSFresults/Evaluation_Table.png' style="width:100%" >

        <p>
		The LPIPS score for the mix model, which is the lowest, indicates a considerable improvement in the quality of
		the rendered output, as shown in the above table, demonstrating how integrating depth truly enhances rendering performance. 
<!--             Here, we show first results for inside-out novel view synthesis. We rendered 500 images of a minecraft room,
            and trained a single SRN with 500k parameters on this dataset. -->
        </p>
	    

    </div>


    <hr>
    <footer>
        <p>Send feedback and questions to <a class="email-link" href="mailto:cheru050@umn.edu">Manideep Cherukuri</a> or 
	    <a class="email-link" href="mailto:chen6242@umn.edu">Claire Chen.</a> </p>
    </footer>

</div><!--/.container-->
<div id='footer' class='vspace-top'>
<div>

<!-- jQuery (necessary for Bootstrap's JavaScript plugins) -->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>
<!-- Include all compiled plugins (below), or include individual files as needed -->
<script src="js/bootstrap.min.js"></script>
</body>
</html>
