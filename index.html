<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description"
          content="Bringing Depth-supervision to Dynamic Neural Radiance Fields with Scene Flows">
    <meta name="author" content="Claire Chen,
				 Mani Deep Cherukuri">

    <title>Bringing Depth-supervision to Dynamic Neural Radiance Fields with Scene Flows</title>
    <!-- Bootstrap core CSS -->
    <link href="css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom styles for this template -->
    <link href="css/offcanvas.css" rel="stylesheet">

<!--    <link rel="icon" href="img/favicon.gif" type="image/gif">-->
</head>

<body>
<div class="container">

    <div class="jumbotron">
	    <h2>Bringing Depth-supervision to Dynamic Neural Radiance Fields with Scene Flows</h2>
<!-- 	    <h2>University of Minnesota</h2> -->
        <p class="abstract">Minnesota Robotics Institute</p>
       	<p class="abstract">University of Minnesota</p>
        <p iclass="authors">
<!--             <a href="http://stanford.edu/~sitzmann/">Vincent Sitzmann</a>,
            <a href="http://zollhoefer.com/">Michael Zollh√∂fer</a>,
            <a href="https://stanford.edu/~gordonwz/">Gordon Wetzstein</a> -->
        </p>

        <p>
            <a class="btn btn-primary" href="https://arxiv.org/abs/2107.02791">DS Paper</a>
            <a class="btn btn-primary" href="https://arxiv.org/abs/2011.13084">NSF Paper</a>
	    <a class="btn btn-primary" href="https://google.com">Code</a>
            <a class="btn btn-primary" href="https://drive.google.com/drive/folders/1OkYgeRcIcLOFu1ft5mRODWNQaPJ0ps90?usp=sharing">Datasets</a>
            <a class="btn btn-primary" href="docs/poster.pdf">Poster</a>
            <a class="btn btn-primary" href="https://drive.google.com/file/d/1KerkXYDJSgl0DOb3IRL3UqCeq3Z3vkqG/view?usp=sharing">Talk</a>
            <a class="btn btn-primary" href="https://rpm-lab.github.io/CSCI5980-Spr23-DeepRob/assets/slides/minn_deeprob_18_NeRF_variants.pdf">Slides</a>
<!--             <a class="btn btn-primary" href="https://drive.google.com/open?id=1IdOywOSLuK6WlkO5_h-ykr3ubeY9eDig">Trained Models</a> -->
    </div>

    <h2>Pitch Video</h2>
    <hr>
	    <div class="col-sm-3">
		<img src='docs/Framework.png' class='img-fluid'>
	    </div>
    <iframe width="884" height="497" class='center' src="https://drive.google.com/file/d/1PQl-FZiWqzs7C13x_Apo7r-yXyHO-GvJ/view?usp=sharing" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

    <hr>
    <p>
	The objective of our work is to investigate the impact of depth-supervised loss on both training time and 
	performance in generation of 3D realistic models using NeRF. While the Depth-supervised NeRF paper demonstrated
	the effectiveness of depth-supervised loss in reducing training time for static scenes, our study aims to 
	evaluate whether this approach can be applied to dynamic scenes. To achieve this goal, we incorporated the 
	DS-loss into the Neural Scene Flow Fields model	and conducted experiments with different loss combinations.
    </p>

    <div class="section">
        <h2>Generalizing Shape & Appearance Priors Across Scenes</h2>
        <hr>
        <div class="gif">
            <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                <source src="img/many_training_cars.mp4" type="video/mp4">
            </video>
        </div>
        <p>
            SRNs explain all 2D observations in 3D, leading to unsupervised, yet explicit, reconstruction of geometry jointly with
            appearance. Normal maps may visualize the reconstructed geometry and make SRNs fully interpretable.
            On the left, you can see the normal maps of the reconstructed
            geometry - note that these are learned fully unsupervised! In the center, you can see novel views generated
            by SRNs, and to the right, the ground-truth views. This model was trained on 50 2D observations each of ~2.5k cars in the Shapenet v2 dataset.
        </p>

        <h2>Camera Pose Extrapolation</h2>
        <hr>
        <div class="gif">
            <video width="40%" playsinline="" autoplay="" loop="" preload="" muted="">
                <source src="img/zoom.mp4" type="video/mp4">
            </video>
            <video width="40%" playsinline=""  autoplay="" loop="" preload="" muted="">
                <source src="img/camera_rotation.mp4" type="video/mp4">
            </video>
        </div>
        <p>
            SRNs generate images without using convolutional neural networks (CNNs) - pixels of a rendered image
            are only connected via the 3D scene representation and can be generated completely independently.
            SRNs can thus be sampled at arbitrary image
            resolutions without retraining, and naturally generalize to completely unseen camera transformations. The model
            that generated the images above was trained on cars, but only on views with a constant distance to
            each car - yet, it flawlessly enables zoom and camera roll, though these transformations were entirely
            unobserved at training time. In contrast, models with black-box neural renderers will fail entirely to
            generate these novel views.
        </p>
        <h2>Instance Interpolation</h2>
        <hr>
        <div class="gif">
            <video width="40%" playsinline=""  autoplay="" loop="" preload="" muted="">
                <source src="img/car_interpolation.mp4" type="video/mp4">
            </video>
            <video width="40%" playsinline="" autoplay="" loop="" preload="" muted="">
                <source src="img/chair_interpolation.mp4" type="video/mp4">
            </video>
        </div>

        <h2>2 View-image Reconstruction</h2>
        <hr>
        <video width="80%" playsinline="" autoplay="" loop="" preload="" muted="" class="gif">
            <source src="img/Single_shot_gif.mp4" type="video/mp4">
        </video>
        <p>
            By generalizing SRNs over a class of scenes, they enable few-shot reconstruction of both shape and geometry
            - a car, for instance, may be reconstructed from only a single observation, enabling almost perfectly
            multi-view consistent novel view generation.
        </p>
        <h2>Non-rigid Deformation</h2>
        <hr>
        <div class="gif">
            <video width="40%" playsinline="" autoplay="" loop="" preload="" muted="">
                <source src="img/face_expressions.mp4" type="video/mp4">
            </video>
            <video width="40%" playsinline="" autoplay="" loop="" preload="" muted="">
                <source src="img/identity_interpolation.mp4" type="video/mp4">
            </video>
        </div>
        <p>
            Because surfaces are parameterized smoothly, SRNs naturally allow for non-rigid deformation. The model above
            was trained on 50 images each of 1000 faces, where we used the ground-truth identity and expression parameters
            as latent codes. A single identity has only been observed with a single facial
            expression. By fixing identity parameters and varying expression parameters,
            SRNs allow for non-rigid deformation of the learned face model, effortlessly generalizing facial expressions across identities (right).
            Similar to the cars and chairs above, interpolation latent vectors yields smooth interpolation of the respective
            identities and expressions (left). Note that all movements are reflected in the normal map as well as the appearance.
        </p>

        <h2>Proof-of-concept: Inside-out Novel View Synthesis</h2>
        <hr>
        <div class="gif">
            <video width="40%" playsinline="" autoplay="" loop="" preload="" muted="">
                <source src="img/minecraft.mp4" type="video/mp4">
            </video>
        </div>
        <p>
            Here, we show first results for inside-out novel view synthesis. We rendered 500 images of a minecraft room,
            and trained a single SRN with 500k parameters on this dataset.
        </p>

        <h2>Motivation</h2>
        <hr>
        <!--<img src="img/teaser.gif" style="width:50%; display:block; margin-right:auto; margin-left:auto; margin-top:-10px;">--!>
        <p>
            <b>Neural Scene Representations.</b> Computer vision has developed many different mathematical models of our 3D world,
            such as voxel grids, point clouds, and meshes. Yet, feats that are easy for a human - such
            as inferring the shape, material, or appearance of a scene from only a single picture - have eluded algorithms
            so far.
        </p>
        <p>
            The advent of deep learning has given rise to <b>neural scene representations</b>.
            Instead of hand-crafting the representation, they learn a feature representation from data.
            However, many of these representations do not explicitly reason about
            geometry and thus do not account for the underlying 3D structure of our world, making them data-inefficient
            and opaque.
        </p>

        <p>
            <b>The trouble with voxel grids.</b> Recent work (<a href="https://vsitzmann.github.io/deepvoxels">including our own</a>) explores voxel grids as a middle ground.
            Features are stored in a 3D grid, and view transformations are hard-coded to enforce 3D structure.
            Voxel grids, however, are an unlikely candidate for the "correct"
            representation, as they require memory that scales cubically with spatial resolution.
            This is acceptable for small objects, but doesn't scale to larger scenes. Lastly, voxel grids do not parameterize
            scene surfaces smoothly, and priors on shape are learnt as joint probabilities of voxel neighborhoods.
        </p>
        <p>
            <b>SRNs.</b> With SRNs, we take steps towards a neural scene representation that is interpretable, allows
            the learning of shape and appearance priors across scenes, and has the potential to scale to large scenes and
            high spatial resolutions.
        </p>



    </div>



    <div class="section">
        <h2>Paper</h2>
        <hr>
        <div>
            <div class="list-group">
                <a href="https://papers.nips.cc/paper/8396-scene-representation-networks-continuous-3d-structure-aware-neural-scene-representations" class="list-group-item">
                    <img src="img/paper_thumbnail.png" style="width:100%; margin-right:-20px; margin-top:-10px;">
                </a>
            </div>
        </div>
    </div>


    <h2>Bibtex</h2>
    <hr>
    <div class="bibtexsection">
        @inproceedings{sitzmann2019srns,
            author = {Sitzmann, Vincent
                      and Zollh{\"o}fer, Michael
                      and Wetzstein, Gordon},
            title = {Scene Representation Networks: Continuous 3D-Structure-Aware Neural Scene Representations},
	    booktitle = {Advances in Neural Information Processing Systems},
            year={2019}
        }
    </div>


    <hr>
    <footer>
        <p>Send feedback and questions to <a href="http://web.stanford.edu/~sitzmann/">Vincent Sitzmann</a></p>
        <p>Thanks to Volodymyr Kuleshov for his website template. &copy; 2017</p>
    </footer>

</div><!--/.container-->
<div id='footer' class='vspace-top'>
<div>

<!-- jQuery (necessary for Bootstrap's JavaScript plugins) -->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>
<!-- Include all compiled plugins (below), or include individual files as needed -->
<script src="js/bootstrap.min.js"></script>
</body>
</html>
